{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip\n",
    "%pip install -U setuptools wheel\n",
    "%pip install -U \"mxnet<2.0.0\" bokeh==2.0.1\n",
    "%pip install autogluon --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /root/.kaggle\n",
    "!touch /root/.kaggle/kaggle.json\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "kaggle_username = \"FILL_IN_USERNAME\"\n",
    "kaggle_key = \"FILL_IN_KEY\"\n",
    "\n",
    "# Save API token the kaggle.json file\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"username\": kaggle_username, \"key\": kaggle_key}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c bike-sharing-demand\n",
    "!unzip -o bike-sharing-demand.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple output of the train dataset to view some of the min/max/varition of the dataset features.\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test pandas dataframe in pandas by reading the csv, remember to parse the datetime!\n",
    "test = pd.read_csv(\"test.csv\", parse_dates=[\"datetime\"])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing as train and test dataset\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(  ).fit(train, target=\"count\")``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict(test)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the `predictions` series to see if there are any negative values\n",
    "describe(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many negative values do we have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set them to zero\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"count\"] = predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission.csv -m \"first raw submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of all features to show the distribution of each one relative to the data. This is part of the exploritory data analysis\n",
    "train.hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport \n",
    "\n",
    "profile = ProfileReport( train_st, title='Pandas profiling report ' , html={'style':{'full_width':True}})\n",
    "\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature\n",
    "train[?] = train[?] * train[?]\n",
    "test[?] = test[?] * test[?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st.drop(['season','weather'],inplace=True,axis=1)\n",
    "train_st.head()\n",
    "test_st.drop(['season','weather'],inplace=True,axis=1)\n",
    "test_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season=pd.get_dummies(train_st['season'],prefix='season')\n",
    "dtrain_stf=pd.concat([train_st,season],axis=1)\n",
    "train_st.head()\n",
    "season=pd.get_dummies(test_st['season'],prefix='season')\n",
    "test_st=pd.concat([test_st,season],axis=1)\n",
    "test_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st[\"hour\"] = [t.hour for t in pd.DatetimeIndex(train_st.datetime)]\n",
    "train_st[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(train_st.datetime)]\n",
    "train_st[\"month\"] = [t.month for t in pd.DatetimeIndex(train_st.datetime)]\n",
    "train_st['year'] = [t.year for t in pd.DatetimeIndex(train_st.datetime)]\n",
    "train_st['year'] = train_st['year'].map({2011:0, 2012:1})\n",
    "train_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View are new feature\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View histogram of all features again now with the hour feature\n",
    "train.hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Random Forest Regressor Model \n",
    "\n",
    "'''\n",
    "sklearn.ensemble.RandomForestRegressor(n_estimators='warn', criterion=’mse’, max_depth=None,\n",
    "                                       min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0,\n",
    "                                       max_features=’auto’, max_leaf_nodes=None,min_impurity_decrease=0.0,\n",
    "                                       min_impurity_split=None, bootstrap=True,oob_score=False, n_jobs=None,\n",
    "                                       random_state=None, verbose=0,warm_start=False)\n",
    "'''\n",
    "\n",
    "RandomForestRegressorModel = RandomForestRegressor(n_estimators=100,max_depth=5, random_state=33)\n",
    "RandomForestRegressorModel.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest Regressor Train Score is : ' , RandomForestRegressorModel.score(x_train, y_train))\n",
    "print('Random Forest Regressor Test Score is : ' , RandomForestRegressorModel.score(x_test, y_test))\n",
    "print('Random Forest Regressor No. of features are : ' , RandomForestRegressorModel.n_features_)\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "#Calculating Prediction\n",
    "y_pred = RandomForestRegressorModel.predict(x_test)\n",
    "print('Predicted Value for Random Forest Regressor is : ' , y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_features.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEValue = mean_squared_error(y_test, y_pred, multioutput='uniform_average') # it can be raw_values\n",
    "print('Mean Squared Error Value is : ', MSEValue)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Mean Absolute Error\n",
    "MAEValue = mean_absolute_error(y_test, y_pred, multioutput='uniform_average') # it can be raw_values\n",
    "print('Mean Absolute Error Value is : ', MAEValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same submitting predictions\n",
    "submission_new_features[\"count\"] = ?\n",
    "submission_new_features.to_csv(\"submission_new_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=RandomForestRegressorModel.predict(test_st.drop('datetime',axis=1))\n",
    "d={'datetime':test['datetime'],'count':pred}\n",
    "ans=pd.DataFrame(d)\n",
    "ans.to_csv('sampleSubmission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission_new_features.csv -m \"new features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_hpo = TabularPredictor(?).fit(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_hpo.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same submitting predictions\n",
    "submission_new_hpo[\"count\"] = ? \n",
    "submission_new_hpo.to_csv(\"submission_new_hpo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo.csv -m \"new features with hyperparameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the top model score from each training run and creating a line plot to show improvement\n",
    "# You can create these in the notebook and save them to PNG or use some other tool (e.g. google sheets, excel)\n",
    "fig = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"initial\", \"add_features\", \"hpo\"],\n",
    "        \"score\": [?, ?, ?]\n",
    "    }\n",
    ").plot(x=\"model\", y=\"score\", figsize=(8, 6)).get_figure()\n",
    "fig.savefig('model_train_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the 3 kaggle scores and creating a line plot to show improvement\n",
    "fig = pd.DataFrame(\n",
    "    {\n",
    "        \"test_eval\": [\"initial\", \"add_features\", \"hpo\"],\n",
    "        \"score\": [?, ?, ?]\n",
    "    }\n",
    ").plot(x=\"test_eval\", y=\"score\", figsize=(8, 6)).get_figure()\n",
    "fig.savefig('model_test_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 hyperparameters we tuned with the kaggle score as the result\n",
    "pd.DataFrame({\n",
    "    \"model\": [\"initial\", \"add_features\", \"hpo\"],\n",
    "    \"hpo1\": [?, ?, ?],\n",
    "    \"hpo2\": [?, ?, ?],\n",
    "    \"hpo3\": [?, ?, ?],\n",
    "    \"score\": [?, ?, ?]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
